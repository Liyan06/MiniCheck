{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Demo\n",
    "\n",
    "This notebook walk you through the steps for doing predictions on the LLM-AggreFact benchmark and obtain the evaluation results shown in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from minicheck.minicheck import MiniCheck\n",
    "\n",
    "\n",
    "df = pd.DataFrame(load_dataset(\"lytang/LLM-AggreFact\")['test'])\n",
    "docs = df.doc.values\n",
    "claims = df.claim.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the fact-checking model\n",
    "There are three models to choose from: ['roberta-large', 'deberta-v3-large', 'flan-t5-large'], where 'flan-t5-large' is our best performing model, reaching GPT-4 performance but 400x cheaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'flan-t5-large'\n",
    "scorer = MiniCheck(model_name=model_name, device=f'cuda:0', cache_dir='./ckpts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the labels\n",
    "Predicting on the entire test set (13K) requires ~10-20 mins, depending on the chosen model and hardware setup. In this demo, we use 'flan-t5-large', which takes ~20 mins (>500 docs/min on average).\n",
    "\n",
    "A GPU with VRAM of 16GB should be sufficient. The GPU usage during the entire prediction process in our local machine is <10 GB most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_label converts the raw probability (raw_prob) into 1/0 using the threshold 0.5\n",
    "pred_label, raw_prob, _, _ = scorer.score(docs=docs, claims=claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance on LLM-AggreFact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>BAcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AggreFact-CNN</td>\n",
       "      <td>69.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AggreFact-XSum</td>\n",
       "      <td>74.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TofuEval-MediaS</td>\n",
       "      <td>73.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TofuEval-MeetB</td>\n",
       "      <td>77.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wice</td>\n",
       "      <td>72.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reveal</td>\n",
       "      <td>86.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ClaimVerify</td>\n",
       "      <td>74.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FactCheck-GPT</td>\n",
       "      <td>74.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ExpertQA</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lfqa</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Average</td>\n",
       "      <td>74.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Dataset  BAcc\n",
       "0     AggreFact-CNN  69.9\n",
       "1    AggreFact-XSum  74.3\n",
       "2   TofuEval-MediaS  73.6\n",
       "3    TofuEval-MeetB  77.3\n",
       "4              Wice  72.2\n",
       "5            Reveal  86.2\n",
       "6       ClaimVerify  74.6\n",
       "7     FactCheck-GPT  74.7\n",
       "8          ExpertQA  59.0\n",
       "9              Lfqa  85.2\n",
       "10          Average  74.7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['preds'] = pred_label\n",
    "result_df = pd.DataFrame(columns=['Dataset', 'BAcc'])\n",
    "for dataset in df.dataset.unique():\n",
    "    sub_df = df[df.dataset == dataset]\n",
    "    bacc = balanced_accuracy_score(sub_df.label, sub_df.preds) * 100\n",
    "    result_df.loc[len(result_df)] = [dataset, bacc]\n",
    "\n",
    "result_df.loc[len(result_df)] = ['Average', result_df.BAcc.mean()]\n",
    "result_df.round(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
