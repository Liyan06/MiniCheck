{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Evaluation Demo\n",
    "\n",
    "This notebook walk you through the steps for doing predictions on the LLM-AggreFact benchmark and obtain the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from minicheck.minicheck import MiniCheck\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "df = pd.DataFrame(load_dataset(\"lytang/LLM-AggreFact\")['test'])\n",
    "docs = df.doc.values\n",
    "claims = df.claim.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the fact-checking model\n",
    "There are four models to choose from: ['roberta-large', 'deberta-v3-large', 'flan-t5-large', 'Bespoke-MiniCheck-7B'] where:\n",
    "\n",
    "(1) `MiniCheck-Flan-T5-Large` is the best fack-checking model with size < 1B and reaches GPT-4 performance. \\\n",
    "(2) `Bespoke-MiniCheck-7B` is the most performant fact-checking model in the MiniCheck series AND \\\n",
    "   it outperforms ALL exisiting specialized fact-checkers and off-the-shelf LLMs regardless of size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Bespoke-MiniCheck-7B'\n",
    "scorer = MiniCheck(model_name=model_name, enable_prefix_caching=False, cache_dir='./ckpts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the labels\n",
    "In this demo, `Bespoke-MiniCheck-7B` (implemented with vLLM) predicting on the entire test set (29K) requires ~50 mins using a single NVIDA A6000 (48GB VRAM). The average throughput > 500 docs/min, same throughput as `MiniCheck-Flan-T5-Large`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_label converts the raw probability (raw_prob) into 1/0 using the threshold 0.5\n",
    "pred_label, raw_prob, _, _ = scorer.score(docs=docs, claims=claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance on LLM-AggreFact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>BAcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AggreFact-CNN</td>\n",
       "      <td>65.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AggreFact-XSum</td>\n",
       "      <td>77.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TofuEval-MediaS</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TofuEval-MeetB</td>\n",
       "      <td>78.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wice</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reveal</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ClaimVerify</td>\n",
       "      <td>75.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FactCheck-GPT</td>\n",
       "      <td>77.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ExpertQA</td>\n",
       "      <td>59.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lfqa</td>\n",
       "      <td>86.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RAGTruth</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Average</td>\n",
       "      <td>77.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Dataset  BAcc\n",
       "0     AggreFact-CNN  65.5\n",
       "1    AggreFact-XSum  77.8\n",
       "2   TofuEval-MediaS  76.0\n",
       "3    TofuEval-MeetB  78.3\n",
       "4              Wice  83.0\n",
       "5            Reveal  88.0\n",
       "6       ClaimVerify  75.3\n",
       "7     FactCheck-GPT  77.7\n",
       "8          ExpertQA  59.2\n",
       "9              Lfqa  86.7\n",
       "10         RAGTruth  84.0\n",
       "11          Average  77.4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['preds'] = pred_label\n",
    "result_df = pd.DataFrame(columns=['Dataset', 'BAcc'])\n",
    "for dataset in df.dataset.unique():\n",
    "    sub_df = df[df.dataset == dataset]\n",
    "    bacc = balanced_accuracy_score(sub_df.label, sub_df.preds) * 100\n",
    "    result_df.loc[len(result_df)] = [dataset, bacc]\n",
    "\n",
    "result_df.loc[len(result_df)] = ['Average', result_df.BAcc.mean()]\n",
    "result_df.round(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
